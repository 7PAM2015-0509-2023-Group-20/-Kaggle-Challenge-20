{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXcKQcNPGDsMvmPR9Zplg+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7PAM2015-0509-2023-Group-20/Kaggle-Challenge-20/blob/main/Group_project_Kaggle_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ4Km6dT5kPw",
        "outputId": "dd010cfc-2a28-4a58-a09d-d81e7d7a8bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.46.0.4.tar.gz (265.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.3/265.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.31.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2024.6.2)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.46.0.4-py2.py3-none-any.whl size=265375577 sha256=164c8053044ae7f427c50b1a964577c12ddbcfae0f947116c94e615da01d059e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/a6/47/8bfeb1026fd65cb8630beb74d8e3bec844f572cf4f336fdd56\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.46.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install h2o"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "mBFIYEghFRqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4t6JLK99FVHq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Importing libraries for H2O AutoML and NetworkX"
      ],
      "metadata": {
        "id": "eXokr0PSFnKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "3dlJ3SGHFpH9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set styles\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "metadata": {
        "id": "fgB51W9RFuyb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load datasets"
      ],
      "metadata": {
        "id": "MswfoUd_F0QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"https://raw.githubusercontent.com/7PAM2015-0509-2023-Group-20/Kaggle-Challenge-20/main/train.csv\")\n",
        "test_data = pd.read_csv(\"https://raw.githubusercontent.com/7PAM2015-0509-2023-Group-20/Kaggle-Challenge-20/main/test.csv\")"
      ],
      "metadata": {
        "id": "iT6pYmBDFy1_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "XIO4K_sf4_e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train dataset shape: {train_data.shape}\")\n",
        "print(f\"Test dataset shape: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSzK8Lyp5Blh",
        "outputId": "e37b4498-6abe-4648-b78c-7d0c5c8001a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (8693, 14)\n",
            "Test dataset shape: (4277, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Duplicate rows in train dataset: {train_data.duplicated().sum()}\")\n",
        "print(f\"Duplicate rows in test dataset: {test_data.duplicated().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xALbN_JO5Gzb",
        "outputId": "4310cbf7-20da-410b-f59b-e477d788c822"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate rows in train dataset: 0\n",
            "Duplicate rows in test dataset: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data types in training data:\")\n",
        "print(train_data.dtypes)"
      ],
      "metadata": {
        "id": "iVeVZKap5fvi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data types in testing data:\")\n",
        "print(test_data.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPYf4Gcn4PWY",
        "outputId": "a14a3293-dfc1-4d2d-f4c6-7b5a86e1cfa5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types in testing data:\n",
            "PassengerId      object\n",
            "HomePlanet       object\n",
            "CryoSleep        object\n",
            "Cabin            object\n",
            "Destination      object\n",
            "Age             float64\n",
            "VIP              object\n",
            "RoomService     float64\n",
            "FoodCourt       float64\n",
            "ShoppingMall    float64\n",
            "Spa             float64\n",
            "VRDeck          float64\n",
            "Name             object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle missing values"
      ],
      "metadata": {
        "id": "Eb3uBGZL4h6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_train = train_data.isnull().sum()[train_data.isnull().sum() > 0]\n",
        "missing_test = test_data.isnull().sum()[test_data.isnull().sum() > 0]\n",
        "print(\"Missing values in train data:\")\n",
        "print(missing_train)\n",
        "print(\"Missing values in test data:\")\n",
        "print(missing_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvJxWeJs4dIV",
        "outputId": "1c0bfff9-b96e-4be0-dac4-0bb748d7e0c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in train data:\n",
            "HomePlanet      201\n",
            "CryoSleep       217\n",
            "Cabin           199\n",
            "Destination     182\n",
            "Age             179\n",
            "VIP             203\n",
            "RoomService     181\n",
            "FoodCourt       183\n",
            "ShoppingMall    208\n",
            "Spa             183\n",
            "VRDeck          188\n",
            "Name            200\n",
            "dtype: int64\n",
            "Missing values in test data:\n",
            "HomePlanet       87\n",
            "CryoSleep        93\n",
            "Cabin           100\n",
            "Destination      92\n",
            "Age              91\n",
            "VIP              93\n",
            "RoomService      82\n",
            "FoodCourt       106\n",
            "ShoppingMall     98\n",
            "Spa             101\n",
            "VRDeck           80\n",
            "Name             94\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering"
      ],
      "metadata": {
        "id": "HMfPy3PX4sfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_features(df):\n",
        "    # PassengerId-based features\n",
        "    df['Group'] = df['PassengerId'].apply(lambda x: x.split('_')[0])\n",
        "    df['Member'] = df['PassengerId'].apply(lambda x: x.split('_')[1])\n",
        "    df['Solo_Travel'] = df['Group'].map(df['Group'].value_counts()) == 1\n",
        "    df['Group_Size'] = df['Group'].map(df['Group'].value_counts())\n",
        "\n",
        "    # Cabin-based features\n",
        "    df['Cabin'].fillna('nan/nan/nan', inplace=True)\n",
        "    df['Cabin_Deck'] = df['Cabin'].apply(lambda x: x.split('/')[0])\n",
        "    df['Cabin_Num'] = df['Cabin'].apply(lambda x: x.split('/')[1])\n",
        "    df['Cabin_Side'] = df['Cabin'].apply(lambda x: x.split('/')[2])\n",
        "    df[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']] = df[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']].replace('nan', np.nan)\n",
        "    df['Cabin_Deck'].fillna(df['Cabin_Deck'].mode()[0], inplace=True)\n",
        "    df['Cabin_Side'].fillna(df['Cabin_Side'].mode()[0], inplace=True)\n",
        "    df['Cabin_Num'] = df['Cabin_Num'].astype(float).fillna(df['Cabin_Num'].median())\n",
        "\n",
        "    df['Cabin_Region'] = pd.cut(df['Cabin_Num'], bins=[0, 300, 600, 900, 1200, 1500, np.inf], labels=[1, 2, 3, 4, 5, 6])\n",
        "\n",
        "    # Age-based features\n",
        "    df['Age_Group'] = pd.cut(df['Age'], bins=[-np.inf, 12, 18, 25, 32, 50, np.inf], labels=['0-12', '13-18', '19-25', '26-32', '33-50', '50+'])\n",
        "\n",
        "    # Expenditure features\n",
        "    df['Total_Expenditure'] = df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
        "    df['No_Spending'] = df['Total_Expenditure'] == 0"
      ],
      "metadata": {
        "id": "nq_bfQdc4ql8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_new_features(train_data)\n",
        "create_new_features(test_data)"
      ],
      "metadata": {
        "id": "fjGj8zOH4zNc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill missing values"
      ],
      "metadata": {
        "id": "s3pwEgyL453y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features = train_data.select_dtypes(include=['object', 'bool']).columns.to_list()\n",
        "num_features = train_data.select_dtypes(include=['int', 'float']).columns.to_list()\n",
        "cat_features.remove('Transported')"
      ],
      "metadata": {
        "id": "YKksA6na42KP"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}